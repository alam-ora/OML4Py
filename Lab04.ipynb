{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Introduction\n",
    "\n",
    "https://www.kaggle.com/janiobachmann/credit-fraud-dealing-with-imbalanced-datasets/notebook\n",
    "\n",
    "In this kernel we will use various predictive models to see how accurate they are in detecting whether a transaction is a normal payment or a fraud. As described in the dataset, the features are scaled and the names of the features are not shown due to privacy reasons. Nevertheless, we can still analyze some important aspects of the dataset. Let's start!\n",
    "Our Goals:\n",
    "Understand the little distribution of the \"little\" data that was provided to us.\n",
    "Create a 50/50 sub-dataframe ratio of \"Fraud\" and \"Non-Fraud\" transactions. (NearMiss Algorithm)\n",
    "Determine the Classifiers we are going to use and decide which one has a higher accuracy.\n",
    "Create a Neural Network and compare the accuracy to our best classifier.\n",
    "Understand common mistaked made with imbalanced datasets.\n",
    "\n",
    "Outline:\n",
    "I. Understanding our data\n",
    "a) Gather Sense of our data\n",
    "\n",
    "II. Preprocessing\n",
    "a) Scaling and Distributing\n",
    "b) Splitting the Data\n",
    "\n",
    "\n",
    "III. Random UnderSampling and Oversampling\n",
    "a) Distributing and Correlating\n",
    "b) Anomaly Detection\n",
    "c) Dimensionality Reduction and Clustering (t-SNE)\n",
    "d) Classifiers\n",
    "e) A Deeper Look into Logistic Regression\n",
    "f) Oversampling with SMOTE\n",
    "\n",
    "\n",
    "IV. Testing \n",
    "a) Testing with Logistic Regression\n",
    "b) Neural Networks Testing (Undersampling vs Oversampling)\n",
    "\n",
    ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "I. Gather Sense of Our Data:\n",
    "\n",
    "The first thing we must do is gather a basic sense of our data. Remember, except for the transaction and amount we dont know what the other columns are (due to privacy reasons). The only thing we know, is that those columns that are unknown have been scaled already.\n",
    "\n",
    "Summary:\n",
    "\n",
    "- The transaction amount is relatively small. \n",
    "- The mean of all the mounts made is approximately USD 88.\n",
    "- There are no \"Null\" values, so we don't have to work on ways to replace values. Most of the transactions were Non-Fraud (99.83%) of the time, while Fraud transactions occurs (017%) of the time in the dataframe.\n",
    "\n",
    "Feature Technicalities:\n",
    "\n",
    "- **PCA Transformation:** The description of the data says that all the features went through a PCA transformation (Dimensionality Reduction technique) (Except for time and amount).\n",
    "- **Scaling:** Keep in mind that in order to implement a PCA transformation features need to be previously scaled. (In this case, all the V features have been scaled or at least that is what we are assuming the people that develop the dataset did.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About the Dataset:\n",
    "ererere\n",
    "\n",
    "The first thing we must do is gather a basic sense of our data. Remember, except for the transaction and amount we dont know what the other columns are (due to privacy reasons). The only thing we know, is that those columns that are unknown have been scaled already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Loading the Data in Oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ssl\n",
    "from six.moves import urllib\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "    \n",
    "FILE_URL = \"https://github.com/nsethi31/Kaggle-Data-Credit-Card-Fraud-Detection/raw/master/creditcard.csv\"\n",
    "DIR_PATH = os.path.join(\"datasets\", \"creditcard\")\n",
    "FILE_PATH = os.path.join(DIR_PATH, \"creditcard.csv\")\n",
    "\n",
    "def fetch_data(url=FILE_URL, dir_path=DIR_PATH, file_path=FILE_PATH):\n",
    "    if not os.path.isdir(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    urllib.request.urlretrieve(url, file_path)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    creditcard=pd.read_csv(FILE_PATH)\n",
    "except:\n",
    "    print(\"Dataset could not be loaded. Is the dataset missing?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's shuffle.. *tell why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard=creditcard.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79284.0</td>\n",
       "      <td>-0.476439</td>\n",
       "      <td>0.973928</td>\n",
       "      <td>1.663094</td>\n",
       "      <td>-0.094903</td>\n",
       "      <td>-0.140560</td>\n",
       "      <td>-0.693951</td>\n",
       "      <td>0.596386</td>\n",
       "      <td>0.081309</td>\n",
       "      <td>-0.631534</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.164060</td>\n",
       "      <td>-0.465665</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.509114</td>\n",
       "      <td>-0.258842</td>\n",
       "      <td>0.041877</td>\n",
       "      <td>0.259124</td>\n",
       "      <td>0.112581</td>\n",
       "      <td>5.34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58052.0</td>\n",
       "      <td>1.207099</td>\n",
       "      <td>-0.300112</td>\n",
       "      <td>-0.311003</td>\n",
       "      <td>0.164989</td>\n",
       "      <td>1.543265</td>\n",
       "      <td>4.058650</td>\n",
       "      <td>-1.140805</td>\n",
       "      <td>1.090853</td>\n",
       "      <td>0.761328</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.115422</td>\n",
       "      <td>-0.279335</td>\n",
       "      <td>-0.040503</td>\n",
       "      <td>1.001063</td>\n",
       "      <td>0.573693</td>\n",
       "      <td>-0.393611</td>\n",
       "      <td>0.074648</td>\n",
       "      <td>0.031567</td>\n",
       "      <td>9.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59570.0</td>\n",
       "      <td>0.769775</td>\n",
       "      <td>-0.583095</td>\n",
       "      <td>1.167355</td>\n",
       "      <td>1.631228</td>\n",
       "      <td>-1.301252</td>\n",
       "      <td>-0.385719</td>\n",
       "      <td>-0.295798</td>\n",
       "      <td>-0.003690</td>\n",
       "      <td>0.675138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262120</td>\n",
       "      <td>0.489147</td>\n",
       "      <td>-0.218436</td>\n",
       "      <td>0.737067</td>\n",
       "      <td>0.390314</td>\n",
       "      <td>-0.282965</td>\n",
       "      <td>0.031618</td>\n",
       "      <td>0.072822</td>\n",
       "      <td>199.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41052.0</td>\n",
       "      <td>-5.755325</td>\n",
       "      <td>-4.558561</td>\n",
       "      <td>0.798001</td>\n",
       "      <td>1.235792</td>\n",
       "      <td>5.986081</td>\n",
       "      <td>-4.735026</td>\n",
       "      <td>-4.833077</td>\n",
       "      <td>-2.646901</td>\n",
       "      <td>1.043391</td>\n",
       "      <td>...</td>\n",
       "      <td>1.100864</td>\n",
       "      <td>-1.620624</td>\n",
       "      <td>-2.248671</td>\n",
       "      <td>0.651485</td>\n",
       "      <td>-1.091354</td>\n",
       "      <td>-0.048695</td>\n",
       "      <td>2.061368</td>\n",
       "      <td>-0.495984</td>\n",
       "      <td>10.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40632.0</td>\n",
       "      <td>1.200556</td>\n",
       "      <td>-0.023438</td>\n",
       "      <td>0.668185</td>\n",
       "      <td>1.159110</td>\n",
       "      <td>-0.388135</td>\n",
       "      <td>0.187676</td>\n",
       "      <td>-0.369061</td>\n",
       "      <td>0.041869</td>\n",
       "      <td>0.761337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049852</td>\n",
       "      <td>0.378687</td>\n",
       "      <td>-0.263214</td>\n",
       "      <td>-0.397683</td>\n",
       "      <td>0.775584</td>\n",
       "      <td>-0.167262</td>\n",
       "      <td>0.059441</td>\n",
       "      <td>0.025381</td>\n",
       "      <td>24.56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>147435.0</td>\n",
       "      <td>-0.529197</td>\n",
       "      <td>0.770453</td>\n",
       "      <td>-0.653286</td>\n",
       "      <td>-1.146514</td>\n",
       "      <td>1.744554</td>\n",
       "      <td>-1.321703</td>\n",
       "      <td>1.852077</td>\n",
       "      <td>-0.331520</td>\n",
       "      <td>-0.851388</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419494</td>\n",
       "      <td>1.125579</td>\n",
       "      <td>-0.597892</td>\n",
       "      <td>-0.356770</td>\n",
       "      <td>0.846133</td>\n",
       "      <td>0.020992</td>\n",
       "      <td>0.017439</td>\n",
       "      <td>0.095091</td>\n",
       "      <td>25.44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>26825.0</td>\n",
       "      <td>-0.822172</td>\n",
       "      <td>0.462946</td>\n",
       "      <td>1.610733</td>\n",
       "      <td>0.367516</td>\n",
       "      <td>0.702717</td>\n",
       "      <td>1.202951</td>\n",
       "      <td>1.118781</td>\n",
       "      <td>0.211590</td>\n",
       "      <td>-0.548454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260470</td>\n",
       "      <td>0.836581</td>\n",
       "      <td>0.085538</td>\n",
       "      <td>-0.655622</td>\n",
       "      <td>-0.060878</td>\n",
       "      <td>-0.271709</td>\n",
       "      <td>-0.042985</td>\n",
       "      <td>-0.123644</td>\n",
       "      <td>115.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>146567.0</td>\n",
       "      <td>-1.134651</td>\n",
       "      <td>2.191609</td>\n",
       "      <td>-0.422256</td>\n",
       "      <td>-0.075439</td>\n",
       "      <td>0.802893</td>\n",
       "      <td>-1.314916</td>\n",
       "      <td>0.734416</td>\n",
       "      <td>-0.666792</td>\n",
       "      <td>-0.865245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.912853</td>\n",
       "      <td>0.155736</td>\n",
       "      <td>-0.533713</td>\n",
       "      <td>-0.159399</td>\n",
       "      <td>1.042906</td>\n",
       "      <td>0.109181</td>\n",
       "      <td>-0.026623</td>\n",
       "      <td>0.061973</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>145759.0</td>\n",
       "      <td>0.521135</td>\n",
       "      <td>0.998573</td>\n",
       "      <td>-0.805170</td>\n",
       "      <td>-0.125699</td>\n",
       "      <td>0.339711</td>\n",
       "      <td>-0.485663</td>\n",
       "      <td>-0.530283</td>\n",
       "      <td>-2.852968</td>\n",
       "      <td>-0.338432</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.603369</td>\n",
       "      <td>-0.539913</td>\n",
       "      <td>0.257533</td>\n",
       "      <td>1.000945</td>\n",
       "      <td>0.435494</td>\n",
       "      <td>0.165260</td>\n",
       "      <td>-0.057689</td>\n",
       "      <td>0.132353</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>78038.0</td>\n",
       "      <td>-0.977145</td>\n",
       "      <td>1.737110</td>\n",
       "      <td>0.569641</td>\n",
       "      <td>0.930104</td>\n",
       "      <td>-0.422085</td>\n",
       "      <td>-0.854281</td>\n",
       "      <td>0.127148</td>\n",
       "      <td>0.603072</td>\n",
       "      <td>-1.054139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266435</td>\n",
       "      <td>0.554824</td>\n",
       "      <td>-0.077953</td>\n",
       "      <td>0.418284</td>\n",
       "      <td>-0.196156</td>\n",
       "      <td>-0.345631</td>\n",
       "      <td>-0.206234</td>\n",
       "      <td>0.037176</td>\n",
       "      <td>1.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0   79284.0 -0.476439  0.973928  1.663094 -0.094903 -0.140560 -0.693951   \n",
       "1   58052.0  1.207099 -0.300112 -0.311003  0.164989  1.543265  4.058650   \n",
       "2   59570.0  0.769775 -0.583095  1.167355  1.631228 -1.301252 -0.385719   \n",
       "3   41052.0 -5.755325 -4.558561  0.798001  1.235792  5.986081 -4.735026   \n",
       "4   40632.0  1.200556 -0.023438  0.668185  1.159110 -0.388135  0.187676   \n",
       "5  147435.0 -0.529197  0.770453 -0.653286 -1.146514  1.744554 -1.321703   \n",
       "6   26825.0 -0.822172  0.462946  1.610733  0.367516  0.702717  1.202951   \n",
       "7  146567.0 -1.134651  2.191609 -0.422256 -0.075439  0.802893 -1.314916   \n",
       "8  145759.0  0.521135  0.998573 -0.805170 -0.125699  0.339711 -0.485663   \n",
       "9   78038.0 -0.977145  1.737110  0.569641  0.930104 -0.422085 -0.854281   \n",
       "\n",
       "         V7        V8        V9  ...         V21       V22       V23  \\\n",
       "0  0.596386  0.081309 -0.631534  ...   -0.164060 -0.465665  0.000306   \n",
       "1 -1.140805  1.090853  0.761328  ...   -0.115422 -0.279335 -0.040503   \n",
       "2 -0.295798 -0.003690  0.675138  ...    0.262120  0.489147 -0.218436   \n",
       "3 -4.833077 -2.646901  1.043391  ...    1.100864 -1.620624 -2.248671   \n",
       "4 -0.369061  0.041869  0.761337  ...    0.049852  0.378687 -0.263214   \n",
       "5  1.852077 -0.331520 -0.851388  ...    0.419494  1.125579 -0.597892   \n",
       "6  1.118781  0.211590 -0.548454  ...    0.260470  0.836581  0.085538   \n",
       "7  0.734416 -0.666792 -0.865245  ...    0.912853  0.155736 -0.533713   \n",
       "8 -0.530283 -2.852968 -0.338432  ...   -1.603369 -0.539913  0.257533   \n",
       "9  0.127148  0.603072 -1.054139  ...    0.266435  0.554824 -0.077953   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Amount  Class  \n",
       "0  0.509114 -0.258842  0.041877  0.259124  0.112581    5.34      0  \n",
       "1  1.001063  0.573693 -0.393611  0.074648  0.031567    9.99      0  \n",
       "2  0.737067  0.390314 -0.282965  0.031618  0.072822  199.00      0  \n",
       "3  0.651485 -1.091354 -0.048695  2.061368 -0.495984   10.99      0  \n",
       "4 -0.397683  0.775584 -0.167262  0.059441  0.025381   24.56      0  \n",
       "5 -0.356770  0.846133  0.020992  0.017439  0.095091   25.44      0  \n",
       "6 -0.655622 -0.060878 -0.271709 -0.042985 -0.123644  115.00      0  \n",
       "7 -0.159399  1.042906  0.109181 -0.026623  0.061973    1.00      0  \n",
       "8  1.000945  0.435494  0.165260 -0.057689  0.132353    1.98      0  \n",
       "9  0.418284 -0.196156 -0.345631 -0.206234  0.037176    1.89      0  \n",
       "\n",
       "[10 rows x 31 columns]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creditcard.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import oml\n",
    "oml.connect(\"pyquser\",\"pyquser\",dsn='(DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=localhost)(PORT=1521))(CONNECT_DATA=(service_name=oaa1)))')\n",
    "\n",
    "oml.isconnected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    oml.drop(table=\"CREDITCARD\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "CC = oml.create(creditcard, table=\"CREDITCARD\", oranumber=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we must do is gather a basic sense of our data. Remember, except for the transaction and amount we dont know what the other columns are (due to privacy reasons). The only thing we know, is that those columns that are unknown have been scaled already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>1.758217e-12</td>\n",
       "      <td>-8.247377e-13</td>\n",
       "      <td>-9.662670e-13</td>\n",
       "      <td>8.321981e-13</td>\n",
       "      <td>1.600333e-13</td>\n",
       "      <td>4.249311e-13</td>\n",
       "      <td>-3.047229e-13</td>\n",
       "      <td>8.673323e-14</td>\n",
       "      <td>-1.179896e-12</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.406660e-13</td>\n",
       "      <td>-5.706109e-13</td>\n",
       "      <td>-9.725809e-13</td>\n",
       "      <td>1.464133e-12</td>\n",
       "      <td>-6.986193e-13</td>\n",
       "      <td>-5.614527e-13</td>\n",
       "      <td>3.332108e-12</td>\n",
       "      <td>-3.518880e-12</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  1.758217e-12 -8.247377e-13 -9.662670e-13  8.321981e-13   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   1.600333e-13  4.249311e-13 -3.047229e-13  8.673323e-14 -1.179896e-12   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "           ...                 V21           V22           V23           V24  \\\n",
       "count      ...        2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean       ...       -3.406660e-13 -5.706109e-13 -9.725809e-13  1.464133e-12   \n",
       "std        ...        7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min        ...       -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%        ...       -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%        ...       -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%        ...        1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max        ...        2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean  -6.986193e-13 -5.614527e-13  3.332108e-12 -3.518880e-12      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CC.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Time',\n",
       " 'V1',\n",
       " 'V2',\n",
       " 'V3',\n",
       " 'V4',\n",
       " 'V5',\n",
       " 'V6',\n",
       " 'V7',\n",
       " 'V8',\n",
       " 'V9',\n",
       " 'V10',\n",
       " 'V11',\n",
       " 'V12',\n",
       " 'V13',\n",
       " 'V14',\n",
       " 'V15',\n",
       " 'V16',\n",
       " 'V17',\n",
       " 'V18',\n",
       " 'V19',\n",
       " 'V20',\n",
       " 'V21',\n",
       " 'V22',\n",
       " 'V23',\n",
       " 'V24',\n",
       " 'V25',\n",
       " 'V26',\n",
       " 'V27',\n",
       " 'V28',\n",
       " 'Amount',\n",
       " 'Class']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CC.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any fields are NULL????\n",
    "df.isnull().sum().max() # Check for an equivalent.. perhaps desrcribe ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes are heavily skewed we need to solve this issue later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Frauds 99.83 % of the dataset\n",
      "Frauds 0.17 % of the dataset\n"
     ]
    }
   ],
   "source": [
    "# The classes are heavily skewed we need to solve this issue later.\n",
    "print('No Frauds', round(CC[(CC['Class'] == 0), 'Class'].count()/len(CC) * 100,2), '% of the dataset')\n",
    "print('Frauds', round(CC[(CC['Class'] == 1), 'Class'].count()/len(CC) * 100,2), '% of the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Notice how imbalanced is our original dataset! Most of the transactions are non-fraud. If we use this dataframe as the base for our predictive models and analysis we might get a lot of errors and our algorithms will probably overfit since it will \"assume\" that most transactions are not fraud. But we don't want our model to assume, we want our model to detect patterns that give signs of fraud!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([284315,    492]),\n",
       " array([0. , 0.5, 1. ]),\n",
       " <BarContainer object of 2 artists>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "oml.graphics.hist(CC['Class'], bins=2, color='gray',\n",
    "                  linestyle='solid', edgecolor='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributions: By seeing the distributions we can have an idea how skewed are these features, we can also see further distributions of the other features. There are techniques that can help the distributions be less skewed which will be implemented in this notebook in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(18,4))\n",
    "\n",
    "# Plot distribution of Transaction Amount\n",
    "amount_val = CC['Amount'].pull()\n",
    "sns.distplot(amount_val, ax=ax[0], color='r')\n",
    "\n",
    "ax[0].set_title('Distribution of Transaction Amount', fontsize=14)\n",
    "ax[0].set_xlim([min(amount_val), max(amount_val)])\n",
    "\n",
    "# Plot distribution of Transaction Amount\n",
    "time_val = CC['Time'].pull()\n",
    "sns.distplot(time_val, ax=ax[1], color='b')\n",
    "\n",
    "ax[1].set_title('Distribution of Transaction Time', fontsize=14)\n",
    "ax[1].set_xlim([min(time_val), max(time_val)])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preparing the Dataset for ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scaling and Distributing**\n",
    "\n",
    "In this phase of our kernel, we will first scale the columns comprise of Time and Amount . Time and amount should be scaled as the other columns. On the other hand, we need to also create a sub sample of the dataframe in order to have an equal amount of Fraud and Non-Fraud cases, helping our algorithms better understand patterns that determines whether a transaction is a fraud or not.\n",
    "What is a sub-Sample?\n",
    "In this scenario, our subsample will be a dataframe with a 50/50 ratio of fraud and non-fraud transactions. Meaning our sub-sample will have the same amount of fraud and non fraud transactions.\n",
    "Why do we create a sub-Sample?\n",
    "In the beginning of this notebook we saw that the original dataframe was heavily imbalanced! Using the original dataframe will cause the following issues:\n",
    "Overfitting: Our classification models will assume that in most cases there are no frauds! What we want for our model is to be certain when a fraud occurs.\n",
    "Wrong Correlations: Although we don't know what the \"V\" features stand for, it will be useful to understand how each of this features influence the result (Fraud or No Fraud) by having an imbalance dataframe we are not able to see the true correlations between the class and features.\n",
    "Summary:\n",
    "Scaled amount and scaled time are the columns with scaled values.\n",
    "There are 492 cases of fraud in our dataset so we can randomly get 492 cases of non-fraud to create our new sub dataframe.\n",
    "We concat the 492 cases of fraud and non fraud, creating a new sub-sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talk about Robust SCaler.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_time_amount (dat):\n",
    "    # Since most of our data has already been scaled we should scale the columns that are left to scale (Amount and Time)\n",
    "    # Apply the Robust Scaling method: (x(i) - mean) / ((75% percentile) - (25% percentile))\n",
    "    # dat[0] is 'Time' and dat[29] is 'Amount'\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # Calculate the 25th & 75th percentiles and means of Time and Amount columns\n",
    "    time_25_pct=np.percentile(dat[:,0], 25, axis=0)\n",
    "    time_75_pct=np.percentile(dat[:,0], 75, axis=0)\n",
    "    time_mean=np.mean(dat[:,0], axis=0)\n",
    "    amount_25_pct=np.percentile(dat[:,29], 25, axis=0)\n",
    "    amount_75_pct=np.percentile(dat[:,29], 75, axis=0)\n",
    "    amount_mean=np.mean(dat[:,29], axis=0)\n",
    "    \n",
    "    dat[:,0] = (dat[:,0] - time_mean)/(time_75_pct - time_25_pct)\n",
    "    dat[:,29] = (dat[:,29] - amount_mean)/(amount_75_pct - amount_25_pct)\n",
    "    \n",
    "    return(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "CC2=oml.table_apply(CC, func=scale_time_amount, func_value=CC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0 -1.113898 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1 -1.113898  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2 -1.113886 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "\n",
       "         V7        V8        V9  ...         V21       V22       V23  \\\n",
       "0  0.239599  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474   \n",
       "1 -0.078803  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288   \n",
       "2  0.791461  0.247676 -1.514654  ...    0.247998  0.771679  0.909412   \n",
       "\n",
       "        V24       V25       V26       V27       V28    Amount  Class  \n",
       "0  0.066928  0.128539 -0.189115  0.133558 -0.021053  0.856150    0.0  \n",
       "1 -0.339846  0.167170  0.125895 -0.008983  0.014724 -1.196948    0.0  \n",
       "2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  4.056597    0.0  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CC2.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equally Distributing and Correlating:\n",
    "Now that we have our dataframe correctly balanced, we can go further with our analysis and data preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation Matrices** Need some work.. \n",
    "\n",
    "Correlation matrices are the essence of understanding our data. We want to know if there are features that influence heavily in whether a specific transaction is a fraud. However, it is important that we use the correct dataframe (subsample) in order for us to see which features have a high positive or negative correlation with regards to fraud transactions.\n",
    "Summary and Explanation:\n",
    "Negative Correlations: V17, V14, V12 and V10 are negatively correlated. Notice how the lower these values are, the more likely the end result will be a fraud transaction.\n",
    "Positive Correlations: V2, V4, V11, and V19 are positively correlated. Notice how the higher these values are, the more likely the end result will be a fraud transaction.\n",
    "BoxPlots: We will use boxplots to have a better understanding of the distribution of these features in fradulent and non fradulent transactions.\n",
    "Note: We have to make sure we use the subsample in our correlation matrix or else our correlation matrix will be affected by the high imbalance between our classes. This occurs due to the high class imbalance in the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = CC2.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corr2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-290-4ce4cf311046>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Imbalanced Correlation Matrix \\n (don't use for reference)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorr2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'coolwarm_r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot_kws\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0max2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SubSample Correlation Matrix \\n (use for reference)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corr2' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x1440 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))\n",
    "\n",
    "sns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)\n",
    "ax1.set_title(\"Imbalanced Correlation Matrix \\n (don't use for reference)\", fontsize=14)\n",
    "\n",
    "sns.heatmap(corr2, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax2)\n",
    "ax2.set_title('SubSample Correlation Matrix \\n (use for reference)', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we use the subsample in our correlation\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))\n",
    "\n",
    "# Entire DataFrame\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)\n",
    "ax1.set_title(\"Imbalanced Correlation Matrix \\n (don't use for reference)\", fontsize=14)\n",
    "\n",
    "\n",
    "sub_sample_corr = new_df.corr()\n",
    "sns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax2)\n",
    "ax2.set_title('SubSample Correlation Matrix \\n (use for reference)', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "oml.graphics.boxplot(new_CC[:, 'Class'], notch=True, showmeans = True)\n",
    "\n",
    "plt.title('V17 vs Class Negative Correlation')\n",
    "plt.ylabel('V17');\n",
    "plt.xlabel('X->');\n",
    "\n",
    "#f, axes = plt.subplots(ncols=4, figsize=(20,4))\n",
    "\n",
    "# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\n",
    "#sns.boxplot(x=\"Class\", y=\"V17\", data=new_df, palette=colors, ax=axes[0])\n",
    "#axes[0].set_title('V17 vs Class Negative Correlation')\n",
    "\n",
    "#sns.boxplot(x=\"Class\", y=\"V14\", data=new_df, palette=colors, ax=axes[1])\n",
    "#axes[1].set_title('V14 vs Class Negative Correlation')\n",
    "\n",
    "\n",
    "#sns.boxplot(x=\"Class\", y=\"V12\", data=new_df, palette=colors, ax=axes[2])\n",
    "#axes[2].set_title('V12 vs Class Negative Correlation')\n",
    "\n",
    "\n",
    "#sns.boxplot(x=\"Class\", y=\"V10\", data=new_df, palette=colors, ax=axes[3])\n",
    "#axes[3].set_title('V10 vs Class Negative Correlation')\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >> Anomaly Detection (Add this later)??\n",
    "\n",
    "Our main aim in this section is to remove \"extreme outliers\" from features that have a high correlation with our classes. This will have a positive impact on the accuracy of our models. \n",
    "\n",
    "\n",
    "Interquartile Range Method:\n",
    "Interquartile Range (IQR): We calculate this by the difference between the 75th percentile and 25th percentile. Our aim is to create a threshold beyond the 75th and 25th percentile that in case some instance pass this threshold the instance will be deleted.\n",
    "Boxplots: Besides easily seeing the 25th and 75th percentiles (both end of the squares) it is also easy to see extreme outliers (points beyond the lower and higher extreme).\n",
    "Outlier Removal Tradeoff:\n",
    "We have to be careful as to how far do we want the threshold for removing outliers. We determine the threshold by multiplying a number (ex: 1.5) by the (Interquartile Range). The higher this threshold is, the less outliers will detect (multiplying by a higher number ex: 3), and the lower this threshold is the more outliers it will detect. \n",
    "\n",
    "\n",
    "The Tradeoff: The lower the threshold the more outliers it will remove however, we want to focus more on \"extreme outliers\" rather than just outliers. Why? because we might run the risk of information loss which will cause our models to have a lower accuracy. You can play with this threshold and see how it affects the accuracy of our classification models.\n",
    "\n",
    "Summary:\n",
    "Visualize Distributions: We first start by visualizing the distribution of the feature we are going to use to eliminate some of the outliers. V14 is the only feature that has a Gaussian distribution compared to features V12 and V10.\n",
    "Determining the threshold: After we decide which number we will use to multiply with the iqr (the lower more outliers removed), we will proceed in determining the upper and lower thresholds by substrating q25 - threshold (lower extreme threshold) and adding q75 + threshold (upper extreme threshold).\n",
    "Conditional Dropping: Lastly, we create a conditional dropping stating that if the \"threshold\" is exceeded in both extremes, the instances will be removed.\n",
    "Boxplot Representation: Visualize through the boxplot that the number of \"extreme outliers\" have been reduced to a considerable amount.\n",
    "Note: After implementing outlier reduction our accuracy has been improved by over 3%! Some outliers can distort the accuracy of our models but remember, we have to avoid an extreme amount of information loss or else our model runs the risk of underfitting.\n",
    "\n",
    "Reference: More information on Interquartile Range Method: How to Use Statistics to Identify Outliers in Data by Jason Brownless (Machine Learning Mastery blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 6))\n",
    "\n",
    "v14_fraud_dist = new_df['V14'].loc[new_df['Class'] == 1].values\n",
    "sns.distplot(v14_fraud_dist,ax=ax1, fit=norm, color='#FB8861')\n",
    "ax1.set_title('V14 Distribution \\n (Fraud Transactions)', fontsize=14)\n",
    "\n",
    "v12_fraud_dist = new_df['V12'].loc[new_df['Class'] == 1].values\n",
    "sns.distplot(v12_fraud_dist,ax=ax2, fit=norm, color='#56F9BB')\n",
    "ax2.set_title('V12 Distribution \\n (Fraud Transactions)', fontsize=14)\n",
    "\n",
    "\n",
    "v10_fraud_dist = new_df['V10'].loc[new_df['Class'] == 1].values\n",
    "sns.distplot(v10_fraud_dist,ax=ax3, fit=norm, color='#C5B3F9')\n",
    "ax3.set_title('V10 Distribution \\n (Fraud Transactions)', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Test/Train Datases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting the Data (Original DataFrame)**\n",
    "\n",
    "Before proceeding with the Random UnderSampling technique we have to separate the orginal dataframe. Why? for testing purposes, remember although we are splitting the data when implementing Random UnderSampling or OverSampling techniques, we want to test our models on the original testing set not on the testing set created by either of these techniques. The main goal is to fit the model either with the dataframes that were undersample and oversample (in order for our models to detect the patterns), and test it on the original testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN, TEST = CC2.split(ratio = (.8, .2), strata_cols = \"Class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(227517, 31), (57290, 31)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[split.shape for split in (TRAIN, TEST)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Under-Sampling:\n",
    "\n",
    "\n",
    "In this phase of the project we will implement \"Random Under Sampling\" which basically consists of removing data in order to have a more balanced dataset and thus avoiding our models to overfitting.\n",
    "\n",
    "Steps:\n",
    "The first thing we have to do is determine how imbalanced is our class (use \"value_counts()\" on the class column to determine the amount for each label)\n",
    "Once we determine how many instances are considered fraud transactions (Fraud = \"1\") , we should bring the non-fraud transactions to the same amount as fraud transactions (assuming we want a 50/50 ratio), this will be equivalent to 492 cases of fraud and 492 cases of non-fraud transactions.\n",
    "After implementing this technique, we have a sub-sample of our dataframe with a 50/50 ratio with regards to our classes. Then the next step we will implement is to shuffle the data to see if our models can maintain a certain accuracy everytime we run this script.\n",
    "Note: The main issue with \"Random Under-Sampling\" is that we run the risk that our classification models will not perform as accurate as we would like to since there is a great deal of information loss (bringing 492 non-fraud transaction from 284,315 non-fraud transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, get all fraud rows\n",
    "TRAIN_SUB=TRAIN[TRAIN['Class'] == 1]\n",
    "\n",
    "# Get the counts of fraud rows in this dataset\n",
    "cnt=TRAIN_SUB[(TRAIN_SUB['Class'] == 1), 'Class'].count()\n",
    "\n",
    "# Get an equal number of non-fraud rows as fraud rows (under-sampling)\n",
    "NON_FRAUD=TRAIN[TRAIN['Class'] == 0].head(cnt)\n",
    "\n",
    "# Append the fraud rows and non-fraud rows\n",
    "TRAIN_SUB=TRAIN_SUB.append(NON_FRAUD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(790, 31)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_SUB.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing this for speeding up performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2=TRAIN_SUB.pull()\n",
    "TRAIN_SUB=oml.push(R2)\n",
    "\n",
    "R3=TRAIN.pull()\n",
    "TRAIN=oml.push(R3)\n",
    "\n",
    "R4=TEST.pull()\n",
    "TEST=oml.push(R4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of the Classes in the subsample dataset :\n",
      "\n",
      "No Frauds 50.0 % of the dataset\n",
      "Frauds 50.0 % of the dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([395, 395]), array([0. , 0.5, 1. ]), <BarContainer object of 2 artists>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Distribution of the Classes in the subsample dataset :\\n')\n",
    "print('No Frauds', round(TRAIN_SUB[(TRAIN_SUB['Class'] == 0), 'Class'].count()/len(TRAIN_SUB) * 100,2), '% of the dataset')\n",
    "print('Frauds', round(TRAIN_SUB[(TRAIN_SUB['Class'] == 1), 'Class'].count()/len(TRAIN_SUB) * 100,2), '% of the dataset')\n",
    "\n",
    "oml.graphics.hist(TRAIN_SUB['Class'], bins=2, color='gray', linestyle='solid', edgecolor='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix (data, test_y):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    return (confusion_matrix(test_y, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifier_performance (conf_matrix, print_ = False):\n",
    "    import oml\n",
    "    import numpy as np\n",
    "    \n",
    "    # Compute model performance stats: accuracy, precision, recall and specificity\n",
    "    accuracy = round((conf_matrix[0][0]+conf_matrix[1][1])/np.sum(conf_matrix), 6)\n",
    "    precision = round((conf_matrix[0][0])/(conf_matrix[0][0]+conf_matrix[0][1]), 6)\n",
    "    recall = round((conf_matrix[0][0])/(conf_matrix[0][0]+conf_matrix[1][0]), 6)\n",
    "    specificity = round((conf_matrix[1][1])/(conf_matrix[1][1]+conf_matrix[0][1]), 6)\n",
    "    \n",
    "    if (print_ == True):\n",
    "        # Print model perf stats\n",
    "        print (\"Accuracy    =\", accuracy)\n",
    "        print (\"Precision   =\", precision)\n",
    "        print (\"Recall      =\", recall)\n",
    "        print (\"Specificity =\", specificity)\n",
    "\n",
    "    return ([accuracy, precision, recall, specificity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_predict (mod, test_x, test_y, print_ = False):\n",
    "    # Predict on the training part of the fold\n",
    "    pred_y = mod.predict(test_x)\n",
    "    \n",
    "    # Get the Confusion Matrix on test predictions\n",
    "    ret = oml.table_apply(data=pred_y, func=create_confusion_matrix, test_y=test_y)\n",
    "    conf_matrix = ret.pull()\n",
    "\n",
    "    # Print the model performance stats from the Matrix\n",
    "    metrics = get_classifier_performance(conf_matrix, print_)\n",
    "    \n",
    "    return (metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_kfold_train (mod, training_set, print_ = False):\n",
    "\n",
    "    # Get folds on the training_set\n",
    "    if (print_ == True): print (\"STEP 1: Creating Folds ...\\n\")\n",
    "    folds = training_set.KFold(n_splits=5, strata_cols='Class')\n",
    "\n",
    "    iter = 0; accuracy = 0; precision = 0; recall = 0; specificity = 0;\n",
    "    # For each fold, fit the model and test\n",
    "    for train, test in folds:\n",
    "        if (print_ == True): print (\"STEP\", iter+2, \": Training with Fold \", iter+1, \"...\")\n",
    "        # Fit the model with the training data.\n",
    "        mod = mod.fit(train.drop('Class'), train['Class'])\n",
    "\n",
    "        # Predict on the training part on this fold\n",
    "        metrics = do_predict(mod, test.drop('Class'), test['Class'])\n",
    "        \n",
    "        accuracy = accuracy + metrics[0]\n",
    "        precision = precision + metrics[1]\n",
    "        recall = recall + metrics[2]\n",
    "        specificity = specificity + metrics[3]\n",
    "        \n",
    "        iter=iter+1\n",
    "        \n",
    "    avg_accuracy = round(accuracy/iter, 4)\n",
    "    avg_precision = round(precision/iter, 4)\n",
    "    avg_recall = round(recall/iter, 4)\n",
    "    avg_specificity = round(specificity/iter, 4)\n",
    "    \n",
    "    # Train the final model on the complete training set\n",
    "    if (print_ == True): print (\"\\nSTEP\", iter+2, \": Training the final model ...\")\n",
    "    mod = mod.fit(training_set.drop('Class'), training_set['Class'])\n",
    "    \n",
    "    if (print_ == True):\n",
    "        # Print the Avg. Accuracy, Precision, Recall, Specificity\n",
    "        print (\"\\n[Accuracy, Precision, Recall, Specificity] =\",\n",
    "               [avg_accuracy, avg_precision, avg_recall, avg_specificity])\n",
    "        \n",
    "    return (mod, [avg_accuracy, avg_precision, avg_recall, avg_specificity])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talk about training with mutiple parameters and cehcking accuracy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "    \n",
    "training_params = pd.DataFrame(columns=['svms_complexity_factor',\n",
    "                                    'svms_regularizer', 'svms_solver', 'Accuracy', 'Precision',\n",
    "                                    'Recall', 'Specificity', 'Score'])\n",
    "\n",
    "for complexity in 'DEFAULT', '0.01', '0.1', '1', '10' :\n",
    "    for regularizer in 'DEFAULT', '\\'SVMS_REGULARIZER_L1\\'', '\\'SVMS_REGULARIZER_L2\\'' :\n",
    "        for solver in 'DEFAULT', '\\'SVMS_SOLVER_SGD\\'', '\\'SVMS_SOLVER_IPM\\'':\n",
    "            temp_row = [(complexity, regularizer, solver, '', '', '', '', '')]\n",
    "            temp_df = pd.DataFrame(temp_row, columns=['svms_complexity_factor',\n",
    "                                    'svms_regularizer', 'svms_solver', 'Accuracy', 'Precision',\n",
    "                                    'Recall', 'Specificity', 'Score'])\n",
    "            training_params = training_params.append(temp_df, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>svms_complexity_factor</th>\n",
       "      <th>svms_regularizer</th>\n",
       "      <th>svms_solver</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEFAULT</td>\n",
       "      <td>DEFAULT</td>\n",
       "      <td>DEFAULT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DEFAULT</td>\n",
       "      <td>DEFAULT</td>\n",
       "      <td>'SVMS_SOLVER_SGD'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DEFAULT</td>\n",
       "      <td>DEFAULT</td>\n",
       "      <td>'SVMS_SOLVER_IPM'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DEFAULT</td>\n",
       "      <td>'SVMS_REGULARIZER_L1'</td>\n",
       "      <td>DEFAULT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DEFAULT</td>\n",
       "      <td>'SVMS_REGULARIZER_L1'</td>\n",
       "      <td>'SVMS_SOLVER_SGD'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DEFAULT</td>\n",
       "      <td>'SVMS_REGULARIZER_L1'</td>\n",
       "      <td>'SVMS_SOLVER_IPM'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DEFAULT</td>\n",
       "      <td>'SVMS_REGULARIZER_L2'</td>\n",
       "      <td>DEFAULT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DEFAULT</td>\n",
       "      <td>'SVMS_REGULARIZER_L2'</td>\n",
       "      <td>'SVMS_SOLVER_SGD'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DEFAULT</td>\n",
       "      <td>'SVMS_REGULARIZER_L2'</td>\n",
       "      <td>'SVMS_SOLVER_IPM'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.01</td>\n",
       "      <td>DEFAULT</td>\n",
       "      <td>DEFAULT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.01</td>\n",
       "      <td>DEFAULT</td>\n",
       "      <td>'SVMS_SOLVER_SGD'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.01</td>\n",
       "      <td>DEFAULT</td>\n",
       "      <td>'SVMS_SOLVER_IPM'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.01</td>\n",
       "      <td>'SVMS_REGULARIZER_L1'</td>\n",
       "      <td>DEFAULT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.01</td>\n",
       "      <td>'SVMS_REGULARIZER_L1'</td>\n",
       "      <td>'SVMS_SOLVER_SGD'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.01</td>\n",
       "      <td>'SVMS_REGULARIZER_L1'</td>\n",
       "      <td>'SVMS_SOLVER_IPM'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.01</td>\n",
       "      <td>'SVMS_REGULARIZER_L2'</td>\n",
       "      <td>DEFAULT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.01</td>\n",
       "      <td>'SVMS_REGULARIZER_L2'</td>\n",
       "      <td>'SVMS_SOLVER_SGD'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.01</td>\n",
       "      <td>'SVMS_REGULARIZER_L2'</td>\n",
       "      <td>'SVMS_SOLVER_IPM'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.1</td>\n",
       "      <td>DEFAULT</td>\n",
       "      <td>DEFAULT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.1</td>\n",
       "      <td>DEFAULT</td>\n",
       "      <td>'SVMS_SOLVER_SGD'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.1</td>\n",
       "      <td>DEFAULT</td>\n",
       "      <td>'SVMS_SOLVER_IPM'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.1</td>\n",
       "      <td>'SVMS_REGULARIZER_L1'</td>\n",
       "      <td>DEFAULT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.1</td>\n",
       "      <td>'SVMS_REGULARIZER_L1'</td>\n",
       "      <td>'SVMS_SOLVER_SGD'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.1</td>\n",
       "      <td>'SVMS_REGULARIZER_L1'</td>\n",
       "      <td>'SVMS_SOLVER_IPM'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.1</td>\n",
       "      <td>'SVMS_REGULARIZER_L2'</td>\n",
       "      <td>DEFAULT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.1</td>\n",
       "      <td>'SVMS_REGULARIZER_L2'</td>\n",
       "      <td>'SVMS_SOLVER_SGD'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.1</td>\n",
       "      <td>'SVMS_REGULARIZER_L2'</td>\n",
       "      <td>'SVMS_SOLVER_IPM'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>DEFAULT</td>\n",
       "      <td>DEFAULT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>DEFAULT</td>\n",
       "      <td>'SVMS_SOLVER_SGD'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>DEFAULT</td>\n",
       "      <td>'SVMS_SOLVER_IPM'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>'SVMS_REGULARIZER_L1'</td>\n",
       "      <td>DEFAULT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>'SVMS_REGULARIZER_L1'</td>\n",
       "      <td>'SVMS_SOLVER_SGD'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>'SVMS_REGULARIZER_L1'</td>\n",
       "      <td>'SVMS_SOLVER_IPM'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>'SVMS_REGULARIZER_L2'</td>\n",
       "      <td>DEFAULT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>'SVMS_REGULARIZER_L2'</td>\n",
       "      <td>'SVMS_SOLVER_SGD'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>'SVMS_REGULARIZER_L2'</td>\n",
       "      <td>'SVMS_SOLVER_IPM'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>10</td>\n",
       "      <td>DEFAULT</td>\n",
       "      <td>DEFAULT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>10</td>\n",
       "      <td>DEFAULT</td>\n",
       "      <td>'SVMS_SOLVER_SGD'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>10</td>\n",
       "      <td>DEFAULT</td>\n",
       "      <td>'SVMS_SOLVER_IPM'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>10</td>\n",
       "      <td>'SVMS_REGULARIZER_L1'</td>\n",
       "      <td>DEFAULT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>10</td>\n",
       "      <td>'SVMS_REGULARIZER_L1'</td>\n",
       "      <td>'SVMS_SOLVER_SGD'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>10</td>\n",
       "      <td>'SVMS_REGULARIZER_L1'</td>\n",
       "      <td>'SVMS_SOLVER_IPM'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>10</td>\n",
       "      <td>'SVMS_REGULARIZER_L2'</td>\n",
       "      <td>DEFAULT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>10</td>\n",
       "      <td>'SVMS_REGULARIZER_L2'</td>\n",
       "      <td>'SVMS_SOLVER_SGD'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>10</td>\n",
       "      <td>'SVMS_REGULARIZER_L2'</td>\n",
       "      <td>'SVMS_SOLVER_IPM'</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   svms_complexity_factor       svms_regularizer        svms_solver Accuracy  \\\n",
       "0                 DEFAULT                DEFAULT            DEFAULT            \n",
       "1                 DEFAULT                DEFAULT  'SVMS_SOLVER_SGD'            \n",
       "2                 DEFAULT                DEFAULT  'SVMS_SOLVER_IPM'            \n",
       "3                 DEFAULT  'SVMS_REGULARIZER_L1'            DEFAULT            \n",
       "4                 DEFAULT  'SVMS_REGULARIZER_L1'  'SVMS_SOLVER_SGD'            \n",
       "5                 DEFAULT  'SVMS_REGULARIZER_L1'  'SVMS_SOLVER_IPM'            \n",
       "6                 DEFAULT  'SVMS_REGULARIZER_L2'            DEFAULT            \n",
       "7                 DEFAULT  'SVMS_REGULARIZER_L2'  'SVMS_SOLVER_SGD'            \n",
       "8                 DEFAULT  'SVMS_REGULARIZER_L2'  'SVMS_SOLVER_IPM'            \n",
       "9                    0.01                DEFAULT            DEFAULT            \n",
       "10                   0.01                DEFAULT  'SVMS_SOLVER_SGD'            \n",
       "11                   0.01                DEFAULT  'SVMS_SOLVER_IPM'            \n",
       "12                   0.01  'SVMS_REGULARIZER_L1'            DEFAULT            \n",
       "13                   0.01  'SVMS_REGULARIZER_L1'  'SVMS_SOLVER_SGD'            \n",
       "14                   0.01  'SVMS_REGULARIZER_L1'  'SVMS_SOLVER_IPM'            \n",
       "15                   0.01  'SVMS_REGULARIZER_L2'            DEFAULT            \n",
       "16                   0.01  'SVMS_REGULARIZER_L2'  'SVMS_SOLVER_SGD'            \n",
       "17                   0.01  'SVMS_REGULARIZER_L2'  'SVMS_SOLVER_IPM'            \n",
       "18                    0.1                DEFAULT            DEFAULT            \n",
       "19                    0.1                DEFAULT  'SVMS_SOLVER_SGD'            \n",
       "20                    0.1                DEFAULT  'SVMS_SOLVER_IPM'            \n",
       "21                    0.1  'SVMS_REGULARIZER_L1'            DEFAULT            \n",
       "22                    0.1  'SVMS_REGULARIZER_L1'  'SVMS_SOLVER_SGD'            \n",
       "23                    0.1  'SVMS_REGULARIZER_L1'  'SVMS_SOLVER_IPM'            \n",
       "24                    0.1  'SVMS_REGULARIZER_L2'            DEFAULT            \n",
       "25                    0.1  'SVMS_REGULARIZER_L2'  'SVMS_SOLVER_SGD'            \n",
       "26                    0.1  'SVMS_REGULARIZER_L2'  'SVMS_SOLVER_IPM'            \n",
       "27                      1                DEFAULT            DEFAULT            \n",
       "28                      1                DEFAULT  'SVMS_SOLVER_SGD'            \n",
       "29                      1                DEFAULT  'SVMS_SOLVER_IPM'            \n",
       "30                      1  'SVMS_REGULARIZER_L1'            DEFAULT            \n",
       "31                      1  'SVMS_REGULARIZER_L1'  'SVMS_SOLVER_SGD'            \n",
       "32                      1  'SVMS_REGULARIZER_L1'  'SVMS_SOLVER_IPM'            \n",
       "33                      1  'SVMS_REGULARIZER_L2'            DEFAULT            \n",
       "34                      1  'SVMS_REGULARIZER_L2'  'SVMS_SOLVER_SGD'            \n",
       "35                      1  'SVMS_REGULARIZER_L2'  'SVMS_SOLVER_IPM'            \n",
       "36                     10                DEFAULT            DEFAULT            \n",
       "37                     10                DEFAULT  'SVMS_SOLVER_SGD'            \n",
       "38                     10                DEFAULT  'SVMS_SOLVER_IPM'            \n",
       "39                     10  'SVMS_REGULARIZER_L1'            DEFAULT            \n",
       "40                     10  'SVMS_REGULARIZER_L1'  'SVMS_SOLVER_SGD'            \n",
       "41                     10  'SVMS_REGULARIZER_L1'  'SVMS_SOLVER_IPM'            \n",
       "42                     10  'SVMS_REGULARIZER_L2'            DEFAULT            \n",
       "43                     10  'SVMS_REGULARIZER_L2'  'SVMS_SOLVER_SGD'            \n",
       "44                     10  'SVMS_REGULARIZER_L2'  'SVMS_SOLVER_IPM'            \n",
       "\n",
       "   Precision Recall Specificity Score  \n",
       "0                                      \n",
       "1                                      \n",
       "2                                      \n",
       "3                                      \n",
       "4                                      \n",
       "5                                      \n",
       "6                                      \n",
       "7                                      \n",
       "8                                      \n",
       "9                                      \n",
       "10                                     \n",
       "11                                     \n",
       "12                                     \n",
       "13                                     \n",
       "14                                     \n",
       "15                                     \n",
       "16                                     \n",
       "17                                     \n",
       "18                                     \n",
       "19                                     \n",
       "20                                     \n",
       "21                                     \n",
       "22                                     \n",
       "23                                     \n",
       "24                                     \n",
       "25                                     \n",
       "26                                     \n",
       "27                                     \n",
       "28                                     \n",
       "29                                     \n",
       "30                                     \n",
       "31                                     \n",
       "32                                     \n",
       "33                                     \n",
       "34                                     \n",
       "35                                     \n",
       "36                                     \n",
       "37                                     \n",
       "38                                     \n",
       "39                                     \n",
       "40                                     \n",
       "41                                     \n",
       "42                                     \n",
       "43                                     \n",
       "44                                     "
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iterative (training_set, training_params):\n",
    "    import pandas as pd\n",
    "    global svm_mod\n",
    "    \n",
    "    all_cols = training_params.columns.values.tolist()\n",
    "    metric_cols = ['Accuracy', 'Precision', 'Recall', 'Specificity', 'Score']\n",
    "    non_metric_cols = list(set(all_cols) - set(metric_cols))\n",
    "    \n",
    "    for index_label, row_ in training_params.iterrows():\n",
    "        string_ = \"\"\n",
    "        for col_ in non_metric_cols:\n",
    "            if training_params.at[index_label, col_] != 'DEFAULT':\n",
    "                string_ = string_ + \",\" + col_ + \"=\" + training_params.at[index_label, col_]\n",
    "        \n",
    "        cmd = \"global svm_mod; svm_mod = oml.svm(\\'classification\\', svms_kernel_function = \\'dbms_data_mining.svms_linear\\'\" + string_ + \")\"\n",
    "\n",
    "        exec (cmd)\n",
    "        svm_mod, metrics = do_kfold_train(svm_mod, training_set)\n",
    "        score = (3*round(2*metrics[0]*metrics[2]/(metrics[0]+metrics[2]),4) + metrics[1]*2 + metrics[3])/6\n",
    "\n",
    "        training_params.at[index_label, 'Accuracy'] = metrics[0]\n",
    "        training_params.at[index_label, 'Precision'] = metrics[1]\n",
    "        training_params.at[index_label, 'Recall'] = metrics[2]\n",
    "        training_params.at[index_label, 'Specificity'] = metrics[3]\n",
    "        training_params.at[index_label, 'Score'] = score\n",
    "\n",
    "        print (\"Iter\", index_label, \":\", metrics, \", Score :\", score)\n",
    "\n",
    "    best_training_params=training_params.loc[training_params['Score'].astype(float).idxmax()]\n",
    "    return (best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 : [0.985, 0.9893, 0.9812, 0.9901] , Score : 0.9863333333333334\n",
      "Iter 1 : [0.9891, 1.0, 0.9794, 1.0] , Score : 0.9921000000000001\n",
      "Iter 2 : [0.985, 0.9893, 0.9812, 0.9901] , Score : 0.9863333333333334\n",
      "Iter 3 : [0.985, 0.9893, 0.9812, 0.9901] , Score : 0.9863333333333334\n",
      "Iter 4 : [0.9723, 0.9574, 0.988, 0.9576] , Score : 0.9687833333333334\n",
      "Iter 5 : [0.985, 0.9893, 0.9812, 0.9901] , Score : 0.9863333333333334\n",
      "Iter 6 : [0.985, 0.9893, 0.9812, 0.9901] , Score : 0.9863333333333334\n",
      "Iter 7 : [0.9891, 1.0, 0.9794, 1.0] , Score : 0.9921000000000001\n",
      "Iter 8 : [0.985, 0.9893, 0.9812, 0.9901] , Score : 0.9863333333333334\n",
      "Iter 9 : [0.9709, 0.9974, 0.949, 0.9975] , Score : 0.9786166666666668\n",
      "Iter 10 : [0.88, 1.0, 0.8075, 1.0] , Score : 0.9211\n",
      "Iter 11 : [0.9709, 0.9974, 0.949, 0.9975] , Score : 0.9786166666666668\n",
      "Iter 12 : [0.9709, 0.9974, 0.949, 0.9975] , Score : 0.9786166666666668\n",
      "Iter 13 : [0.8813, 1.0, 0.8091, 1.0] , Score : 0.9218500000000001\n",
      "Iter 14 : [0.9709, 0.9974, 0.949, 0.9975] , Score : 0.9786166666666668\n",
      "Iter 15 : [0.9709, 0.9974, 0.949, 0.9975] , Score : 0.9786166666666668\n",
      "Iter 16 : [0.88, 1.0, 0.8075, 1.0] , Score : 0.9211\n",
      "Iter 17 : [0.9709, 0.9974, 0.949, 0.9975] , Score : 0.9786166666666668\n",
      "Iter 18 : [0.9844, 0.9974, 0.9733, 0.9975] , Score : 0.9881166666666665\n",
      "Iter 19 : [0.8826, 0.9974, 0.8129, 0.9967] , Score : 0.9217333333333332\n",
      "Iter 20 : [0.9844, 0.9974, 0.9733, 0.9975] , Score : 0.9881166666666665\n",
      "Iter 21 : [0.9844, 0.9974, 0.9733, 0.9975] , Score : 0.9881166666666665\n",
      "Iter 22 : [0.8969, 0.9974, 0.8328, 0.9969] , Score : 0.9304666666666667\n",
      "Iter 23 : [0.9844, 0.9974, 0.9733, 0.9975] , Score : 0.9881166666666665\n",
      "Iter 24 : [0.9844, 0.9974, 0.9733, 0.9975] , Score : 0.9881166666666665\n",
      "Iter 25 : [0.8826, 0.9974, 0.8129, 0.9967] , Score : 0.9217333333333332\n",
      "Iter 26 : [0.9844, 0.9974, 0.9733, 0.9975] , Score : 0.9881166666666665\n",
      "Iter 27 : [0.9879, 0.9974, 0.9796, 0.9975] , Score : 0.9905666666666667\n",
      "Iter 28 : [0.899, 0.9974, 0.8349, 0.9969] , Score : 0.9315166666666667\n",
      "Iter 29 : [0.9879, 0.9974, 0.9796, 0.9975] , Score : 0.9905666666666667\n",
      "Iter 30 : [0.9879, 0.9974, 0.9796, 0.9975] , Score : 0.9905666666666667\n",
      "Iter 31 : [0.9556, 0.9974, 0.9247, 0.9973] , Score : 0.9686333333333333\n",
      "Iter 32 : [0.9879, 0.9974, 0.9796, 0.9975] , Score : 0.9905666666666667\n",
      "Iter 33 : [0.9879, 0.9974, 0.9796, 0.9975] , Score : 0.9905666666666667\n",
      "Iter 34 : [0.899, 0.9974, 0.8349, 0.9969] , Score : 0.9315166666666667\n",
      "Iter 35 : [0.9879, 0.9974, 0.9796, 0.9975] , Score : 0.9905666666666667\n",
      "Iter 36 : [0.985, 0.9893, 0.9812, 0.9901] , Score : 0.9863333333333334\n",
      "Iter 37 : [0.9709, 0.9974, 0.9496, 0.9975] , Score : 0.9787666666666667\n",
      "Iter 38 : [0.985, 0.9893, 0.9812, 0.9901] , Score : 0.9863333333333334\n",
      "Iter 39 : [0.985, 0.9893, 0.9812, 0.9901] , Score : 0.9863333333333334\n",
      "Iter 40 : [0.965, 0.9974, 0.9393, 0.9974] , Score : 0.9746999999999999\n",
      "Iter 41 : [0.985, 0.9893, 0.9812, 0.9901] , Score : 0.9863333333333334\n",
      "Iter 42 : [0.985, 0.9893, 0.9812, 0.9901] , Score : 0.9863333333333334\n",
      "Iter 43 : [0.9709, 0.9974, 0.9496, 0.9975] , Score : 0.9787666666666667\n",
      "Iter 44 : [0.985, 0.9893, 0.9812, 0.9901] , Score : 0.9863333333333334\n"
     ]
    }
   ],
   "source": [
    "best_training_params = train_iterative(TRAIN_SUB, training_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the best parameters with top K1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svms_complexity_factor              DEFAULT\n",
       "svms_regularizer                    DEFAULT\n",
       "svms_solver               'SVMS_SOLVER_SGD'\n",
       "Accuracy                             0.9891\n",
       "Precision                                 1\n",
       "Recall                               0.9794\n",
       "Specificity                               1\n",
       "Score                                0.9921\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_training_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train the full dataset with the paramenters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_mod = oml.svm('classification', svms_kernel_function='SVMS_GAUSSIAN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_mod = svm_mod.fit(TRAIN.drop('Class'), TRAIN['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = do_predict(svm_mod, TEST.drop('Class'), TEST['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.999249, 0.999878, 0.999371, 0.897059]"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_mod, metrics = do_kfold_train(svm_mod, TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train on Sub first and check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Creating Folds ...\n",
      "\n",
      "STEP 2 : Training with Fold  1 ...\n",
      "STEP 3 : Training with Fold  2 ...\n",
      "STEP 4 : Training with Fold  3 ...\n",
      "STEP 5 : Training with Fold  4 ...\n",
      "STEP 6 : Training with Fold  5 ...\n",
      "\n",
      "STEP 7 : Training the final model ...\n",
      "\n",
      "[Average Accuracy, Average Precision, Average Recall, Average Specificity] = [0.9879, 0.9974, 0.9796, 0.9975]\n"
     ]
    }
   ],
   "source": [
    "# Create an oml SVM model object with hyperparameter settings\n",
    "svm_mod, metrics = do_kfold_train(svm_mod, TRAIN_SUB, print_ = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9879, 0.9974, 0.9796, 0.9975]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy    = 0.99918\n",
      "Precision   = 0.999895\n",
      "Recall      = 0.999284\n",
      "Specificity = 0.903226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.99918, 0.999895, 0.999284, 0.903226]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on Test Dataset\n",
    "do_predict(svm_mod, TEST.drop('Class'), TEST['Class'], print_ = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Printing Average scores across all folds:\n",
      "Average Accuracy = 0.9992\n",
      "Average Precision = 0.9998\n",
      "Average Recall = 0.9994\n",
      "Average Specificity = 0.8448\n"
     ]
    }
   ],
   "source": [
    "# Create an oml SVM model object with hyperparameter settings\n",
    "svm_mod = do_kfold_train(svm_mod, TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy    = 0.99918\n",
      "Precision   = 0.999895\n",
      "Recall      = 0.999284\n",
      "Specificity = 0.903226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.99918, 0.999895, 0.999284, 0.903226]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on Test Dataset\n",
    "do_predict(svm_mod, TEST.drop('Class'), TEST['Class'], print_ = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement simple classifiers\n",
    "\n",
    "classifiers = {\n",
    "    \"LogisiticRegression\": LogisticRegression(),\n",
    "    \"KNearest\": KNeighborsClassifier(),\n",
    "    \"Support Vector Classifier\": SVC(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wow our scores are getting even high scores even when applying cross validation.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "for key, classifier in classifiers.items():\n",
    "    classifier.fit(X_train, y_train)\n",
    "    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GridSearchCV to find the best parameters.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Logistic Regression \n",
    "log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "\n",
    "\n",
    "grid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\n",
    "grid_log_reg.fit(X_train, y_train)\n",
    "# We automatically get the logistic regression with the best parameters.\n",
    "log_reg = grid_log_reg.best_estimator_\n",
    "\n",
    "knears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
    "\n",
    "grid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\n",
    "grid_knears.fit(X_train, y_train)\n",
    "# KNears best estimator\n",
    "knears_neighbors = grid_knears.best_estimator_\n",
    "\n",
    "# Support Vector Classifier\n",
    "svc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\n",
    "grid_svc = GridSearchCV(SVC(), svc_params)\n",
    "grid_svc.fit(X_train, y_train)\n",
    "\n",
    "# SVC best estimator\n",
    "svc = grid_svc.best_estimator_\n",
    "\n",
    "# DecisionTree Classifier\n",
    "tree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n",
    "              \"min_samples_leaf\": list(range(5,7,1))}\n",
    "grid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\n",
    "grid_tree.fit(X_train, y_train)\n",
    "\n",
    "# tree best estimator\n",
    "tree_clf = grid_tree.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting Case\n",
    "\n",
    "log_reg_score = cross_val_score(log_reg, X_train, y_train, cv=5)\n",
    "print('Logistic Regression Cross Validation Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n",
    "\n",
    "\n",
    "knears_score = cross_val_score(knears_neighbors, X_train, y_train, cv=5)\n",
    "print('Knears Neighbors Cross Validation Score', round(knears_score.mean() * 100, 2).astype(str) + '%')\n",
    "\n",
    "svc_score = cross_val_score(svc, X_train, y_train, cv=5)\n",
    "print('Support Vector Classifier Cross Validation Score', round(svc_score.mean() * 100, 2).astype(str) + '%')\n",
    "\n",
    "tree_score = cross_val_score(tree_clf, X_train, y_train, cv=5)\n",
    "print('DecisionTree Classifier Cross Validation Score', round(tree_score.mean() * 100, 2).astype(str) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will undersample during cross validating\n",
    "undersample_X = df.drop('Class', axis=1)\n",
    "undersample_y = df['Class']\n",
    "\n",
    "for train_index, test_index in sss.split(undersample_X, undersample_y):\n",
    "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    undersample_Xtrain, undersample_Xtest = undersample_X.iloc[train_index], undersample_X.iloc[test_index]\n",
    "    undersample_ytrain, undersample_ytest = undersample_y.iloc[train_index], undersample_y.iloc[test_index]\n",
    "    \n",
    "undersample_Xtrain = undersample_Xtrain.values\n",
    "undersample_Xtest = undersample_Xtest.values\n",
    "undersample_ytrain = undersample_ytrain.values\n",
    "undersample_ytest = undersample_ytest.values \n",
    "\n",
    "undersample_accuracy = []\n",
    "undersample_precision = []\n",
    "undersample_recall = []\n",
    "undersample_f1 = []\n",
    "undersample_auc = []\n",
    "\n",
    "# Implementing NearMiss Technique \n",
    "# Distribution of NearMiss (Just to see how it distributes the labels we won't use these variables)\n",
    "X_nearmiss, y_nearmiss = NearMiss().fit_sample(undersample_X.values, undersample_y.values)\n",
    "print('NearMiss Label Distribution: {}'.format(Counter(y_nearmiss)))\n",
    "# Cross Validating the right way\n",
    "\n",
    "for train, test in sss.split(undersample_Xtrain, undersample_ytrain):\n",
    "    undersample_pipeline = imbalanced_make_pipeline(NearMiss(sampling_strategy='majority'), log_reg) # SMOTE happens during Cross Validation not before..\n",
    "    undersample_model = undersample_pipeline.fit(undersample_Xtrain[train], undersample_ytrain[train])\n",
    "    undersample_prediction = undersample_model.predict(undersample_Xtrain[test])\n",
    "    \n",
    "    undersample_accuracy.append(undersample_pipeline.score(original_Xtrain[test], original_ytrain[test]))\n",
    "    undersample_precision.append(precision_score(original_ytrain[test], undersample_prediction))\n",
    "    undersample_recall.append(recall_score(original_ytrain[test], undersample_prediction))\n",
    "    undersample_f1.append(f1_score(original_ytrain[test], undersample_prediction))\n",
    "    undersample_auc.append(roc_auc_score(original_ytrain[test], undersample_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's Plot LogisticRegression Learning Curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(estimator1, estimator2, estimator3, estimator4, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(20,14), sharey=True)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    # First Estimator\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"#ff9124\")\n",
    "    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n",
    "    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n",
    "             label=\"Training score\")\n",
    "    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n",
    "             label=\"Cross-validation score\")\n",
    "    ax1.set_title(\"Logistic Regression Learning Curve\", fontsize=14)\n",
    "    ax1.set_xlabel('Training size (m)')\n",
    "    ax1.set_ylabel('Score')\n",
    "    ax1.grid(True)\n",
    "    ax1.legend(loc=\"best\")\n",
    "    \n",
    "    # Second Estimator \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator2, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    ax2.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"#ff9124\")\n",
    "    ax2.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n",
    "    ax2.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n",
    "             label=\"Training score\")\n",
    "    ax2.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n",
    "             label=\"Cross-validation score\")\n",
    "    ax2.set_title(\"Knears Neighbors Learning Curve\", fontsize=14)\n",
    "    ax2.set_xlabel('Training size (m)')\n",
    "    ax2.set_ylabel('Score')\n",
    "    ax2.grid(True)\n",
    "    ax2.legend(loc=\"best\")\n",
    "    \n",
    "    # Third Estimator\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator3, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    ax3.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"#ff9124\")\n",
    "    ax3.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n",
    "    ax3.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n",
    "             label=\"Training score\")\n",
    "    ax3.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n",
    "             label=\"Cross-validation score\")\n",
    "    ax3.set_title(\"Support Vector Classifier \\n Learning Curve\", fontsize=14)\n",
    "    ax3.set_xlabel('Training size (m)')\n",
    "    ax3.set_ylabel('Score')\n",
    "    ax3.grid(True)\n",
    "    ax3.legend(loc=\"best\")\n",
    "    \n",
    "    # Fourth Estimator\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator4, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    ax4.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"#ff9124\")\n",
    "    ax4.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n",
    "    ax4.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n",
    "             label=\"Training score\")\n",
    "    ax4.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n",
    "             label=\"Cross-validation score\")\n",
    "    ax4.set_title(\"Decision Tree Classifier \\n Learning Curve\", fontsize=14)\n",
    "    ax4.set_xlabel('Training size (m)')\n",
    "    ax4.set_ylabel('Score')\n",
    "    ax4.grid(True)\n",
    "    ax4.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\n",
    "plot_learning_curve(log_reg, knears_neighbors, svc, tree_clf, X_train, y_train, (0.87, 1.01), cv=cv, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "# Create a DataFrame with all the scores and the classifiers names.\n",
    "\n",
    "log_reg_pred = cross_val_predict(log_reg, X_train, y_train, cv=5,\n",
    "                             method=\"decision_function\")\n",
    "\n",
    "knears_pred = cross_val_predict(knears_neighbors, X_train, y_train, cv=5)\n",
    "\n",
    "svc_pred = cross_val_predict(svc, X_train, y_train, cv=5,\n",
    "                             method=\"decision_function\")\n",
    "\n",
    "tree_pred = cross_val_predict(tree_clf, X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print('Logistic Regression: ', roc_auc_score(y_train, log_reg_pred))\n",
    "print('KNears Neighbors: ', roc_auc_score(y_train, knears_pred))\n",
    "print('Support Vector Classifier: ', roc_auc_score(y_train, svc_pred))\n",
    "print('Decision Tree Classifier: ', roc_auc_score(y_train, tree_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_fpr, log_tpr, log_thresold = roc_curve(y_train, log_reg_pred)\n",
    "knear_fpr, knear_tpr, knear_threshold = roc_curve(y_train, knears_pred)\n",
    "svc_fpr, svc_tpr, svc_threshold = roc_curve(y_train, svc_pred)\n",
    "tree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, tree_pred)\n",
    "\n",
    "\n",
    "def graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr):\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.title('ROC Curve \\n Top 4 Classifiers', fontsize=18)\n",
    "    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train, log_reg_pred)))\n",
    "    plt.plot(knear_fpr, knear_tpr, label='KNears Neighbors Classifier Score: {:.4f}'.format(roc_auc_score(y_train, knears_pred)))\n",
    "    plt.plot(svc_fpr, svc_tpr, label='Support Vector Classifier Score: {:.4f}'.format(roc_auc_score(y_train, svc_pred)))\n",
    "    plt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y_train, tree_pred)))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([-0.01, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n",
    "                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n",
    "                )\n",
    "    plt.legend()\n",
    "    \n",
    "graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Deeper Look into LogisticRegression:\n",
    "In this section we will ive a deeper look into the logistic regression classifier.\n",
    "\n",
    "Terms:\n",
    "True Positives: Correctly Classified Fraud Transactions\n",
    "False Positives: Incorrectly Classified Fraud Transactions\n",
    "True Negative: Correctly Classified Non-Fraud Transactions\n",
    "False Negative: Incorrectly Classified Non-Fraud Transactions\n",
    "Precision: True Positives/(True Positives + False Positives)\n",
    "Recall: True Positives/(True Positives + False Negatives)\n",
    "Precision as the name says, says how precise (how sure) is our model in detecting fraud transactions while recall is the amount of fraud cases our model is able to detect.\n",
    "Precision/Recall Tradeoff: The more precise (selective) our model is, the less cases it will detect. Example: Assuming that our model has a precision of 95%, Let's say there are only 5 fraud cases in which the model is 95% precise or more that these are fraud cases. Then let's say there are 5 more cases that our model considers 90% to be a fraud case, if we lower the precision there are more cases that our model will be able to detect.\n",
    "Summary:\n",
    "Precision starts to descend between 0.90 and 0.92 nevertheless, our precision score is still pretty high and still we have a descent recall score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_roc_curve(log_fpr, log_tpr):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.title('Logistic Regression ROC Curve', fontsize=16)\n",
    "    plt.plot(log_fpr, log_tpr, 'b-', linewidth=2)\n",
    "    plt.plot([0, 1], [0, 1], 'r--')\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "    plt.axis([-0.01,1,0,1])\n",
    "    \n",
    "    \n",
    "logistic_roc_curve(log_fpr, log_tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, threshold = precision_recall_curve(y_train, log_reg_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n",
    "y_pred = log_reg.predict(X_train)\n",
    "\n",
    "# Overfitting Case\n",
    "print('---' * 45)\n",
    "print('Overfitting: \\n')\n",
    "print('Recall Score: {:.2f}'.format(recall_score(y_train, y_pred)))\n",
    "print('Precision Score: {:.2f}'.format(precision_score(y_train, y_pred)))\n",
    "print('F1 Score: {:.2f}'.format(f1_score(y_train, y_pred)))\n",
    "print('Accuracy Score: {:.2f}'.format(accuracy_score(y_train, y_pred)))\n",
    "print('---' * 45)\n",
    "\n",
    "# How it should look like\n",
    "print('---' * 45)\n",
    "print('How it should be:\\n')\n",
    "print(\"Accuracy Score: {:.2f}\".format(np.mean(undersample_accuracy)))\n",
    "print(\"Precision Score: {:.2f}\".format(np.mean(undersample_precision)))\n",
    "print(\"Recall Score: {:.2f}\".format(np.mean(undersample_recall)))\n",
    "print(\"F1 Score: {:.2f}\".format(np.mean(undersample_f1)))\n",
    "print('---' * 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersample_y_score = log_reg.decision_function(original_Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "undersample_average_precision = average_precision_score(original_ytest, undersample_y_score)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      undersample_average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(original_ytest, undersample_y_score)\n",
    "\n",
    "plt.step(recall, precision, color='#004a93', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='#48a6ff')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('UnderSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n",
    "          undersample_average_precision), fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE Technique (Over-Sampling):\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/rikunert/SMOTE_visualisation/master/SMOTE_R_visualisation_3.png\", width=800> SMOTE stands for Synthetic Minority Over-sampling Technique. Unlike Random UnderSampling, SMOTE creates new synthetic points in order to have an equal balance of the classes. This is another alternative for solving the \"class imbalance problems\". \n",
    "\n",
    "\n",
    "Understanding SMOTE:\n",
    "\n",
    "Solving the Class Imbalance: SMOTE creates synthetic points from the minority class in order to reach an equal balance between the minority and majority class.\n",
    "Location of the synthetic points: SMOTE picks the distance between the closest neighbors of the minority class, in between these distances it creates synthetic points.\n",
    "Final Effect: More information is retained since we didn't have to delete any rows unlike in random undersampling.\n",
    "Accuracy || Time Tradeoff: Although it is likely that SMOTE will be more accurate than random under-sampling, it will take more time to train since no rows are eliminated as previously stated.\n",
    "Cross Validation Overfitting Mistake:\n",
    "\n",
    "Overfitting during Cross Validation:\n",
    "In our undersample analysis I want to show you a common mistake I made that I want to share with all of you. It is simple, if you want to undersample or oversample your data you should not do it before cross validating. Why because you will be directly influencing the validation set before implementing cross-validation causing a \"data leakage\" problem. In the following section you will see amazing precision and recall scores but in reality our data is overfitting!\n",
    "\n",
    "The Wrong Way:\n",
    "\n",
    "\n",
    "\n",
    "As mentioned previously, if we get the minority class (\"Fraud) in our case, and create the synthetic points before cross validating we have a certain influence on the \"validation set\" of the cross validation process. Remember how cross validation works, let's assume we are splitting the data into 5 batches, 4/5 of the dataset will be the training set while 1/5 will be the validation set. The test set should not be touched! For that reason, we have to do the creation of synthetic datapoints \"during\" cross-validation and not before, just like below: \n",
    "\n",
    "The Right Way:\n",
    "\n",
    "\n",
    "As you see above, SMOTE occurs \"during\" cross validation and not \"prior\" to the cross validation process. Synthetic data are created only for the training set without affecting the validation set.\n",
    "\n",
    "References:\n",
    "\n",
    "DEALING WITH IMBALANCED DATA: UNDERSAMPLING, OVERSAMPLING AND PROPER CROSS-VALIDATION\n",
    "SMOTE explained for noobs\n",
    "Machine Learning - Over-& Undersampling - Python/ Scikit/ Scikit-Imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "\n",
    "\n",
    "print('Length of X (train): {} | Length of y (train): {}'.format(len(original_Xtrain), len(original_ytrain)))\n",
    "print('Length of X (test): {} | Length of y (test): {}'.format(len(original_Xtest), len(original_ytest)))\n",
    "\n",
    "# List to append the score and then find the average\n",
    "accuracy_lst = []\n",
    "precision_lst = []\n",
    "recall_lst = []\n",
    "f1_lst = []\n",
    "auc_lst = []\n",
    "\n",
    "# Classifier with optimal parameters\n",
    "# log_reg_sm = grid_log_reg.best_estimator_\n",
    "log_reg_sm = LogisticRegression()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rand_log_reg = RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=4)\n",
    "\n",
    "\n",
    "# Implementing SMOTE Technique \n",
    "# Cross Validating the right way\n",
    "# Parameters\n",
    "log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "for train, test in sss.split(original_Xtrain, original_ytrain):\n",
    "    pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_log_reg) # SMOTE happens during Cross Validation not before..\n",
    "    model = pipeline.fit(original_Xtrain[train], original_ytrain[train])\n",
    "    best_est = rand_log_reg.best_estimator_\n",
    "    prediction = best_est.predict(original_Xtrain[test])\n",
    "    \n",
    "    accuracy_lst.append(pipeline.score(original_Xtrain[test], original_ytrain[test]))\n",
    "    precision_lst.append(precision_score(original_ytrain[test], prediction))\n",
    "    recall_lst.append(recall_score(original_ytrain[test], prediction))\n",
    "    f1_lst.append(f1_score(original_ytrain[test], prediction))\n",
    "    auc_lst.append(roc_auc_score(original_ytrain[test], prediction))\n",
    "    \n",
    "print('---' * 45)\n",
    "print('')\n",
    "print(\"accuracy: {}\".format(np.mean(accuracy_lst)))\n",
    "print(\"precision: {}\".format(np.mean(precision_lst)))\n",
    "print(\"recall: {}\".format(np.mean(recall_lst)))\n",
    "print(\"f1: {}\".format(np.mean(f1_lst)))\n",
    "print('---' * 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['No Fraud', 'Fraud']\n",
    "smote_prediction = best_est.predict(original_Xtest)\n",
    "print(classification_report(original_ytest, smote_prediction, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = best_est.decision_function(original_Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision = average_precision_score(original_ytest, y_score)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(original_ytest, y_score)\n",
    "\n",
    "plt.step(recall, precision, color='r', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='#F59B00')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('OverSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n",
    "          average_precision), fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE Technique (OverSampling) After splitting and Cross Validating\n",
    "sm = SMOTE(ratio='minority', random_state=42)\n",
    "# Xsm_train, ysm_train = sm.fit_sample(X_train, y_train)\n",
    "\n",
    "\n",
    "# This will be the data were we are going to \n",
    "Xsm_train, ysm_train = sm.fit_sample(original_Xtrain, original_ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We Improve the score by 2% points approximately \n",
    "# Implement GridSearchCV and the other models.\n",
    "\n",
    "# Logistic Regression\n",
    "t0 = time.time()\n",
    "log_reg_sm = grid_log_reg.best_estimator_\n",
    "log_reg_sm.fit(Xsm_train, ysm_train)\n",
    "t1 = time.time()\n",
    "print(\"Fitting oversample data took :{} sec\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Data with Logistic Regression:\n",
    "Confusion Matrix:\n",
    "Positive/Negative: Type of Class (label) [\"No\", \"Yes\"] True/False: Correctly or Incorrectly classified by the model.\n",
    "\n",
    "\n",
    "True Negatives (Top-Left Square): This is the number of correctly classifications of the \"No\" (No Fraud Detected) class. \n",
    "\n",
    "\n",
    "False Negatives (Top-Right Square): This is the number of incorrectly classifications of the \"No\"(No Fraud Detected) class. \n",
    "\n",
    "\n",
    "False Positives (Bottom-Left Square): This is the number of incorrectly classifications of the \"Yes\" (Fraud Detected) class \n",
    "\n",
    "\n",
    "True Positives (Bottom-Right Square): This is the number of correctly classifications of the \"Yes\" (Fraud Detected) class.\n",
    "\n",
    "Summary:\n",
    "\n",
    "Random UnderSampling: We will evaluate the final performance of the classification models in the random undersampling subset. Keep in mind that this is not the data from the original dataframe.\n",
    "Classification Models: The models that performed the best were logistic regression and support vector classifier (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Logistic Regression fitted using SMOTE technique\n",
    "y_pred_log_reg = log_reg_sm.predict(X_test)\n",
    "\n",
    "# Other models fitted with UnderSampling\n",
    "y_pred_knear = knears_neighbors.predict(X_test)\n",
    "y_pred_svc = svc.predict(X_test)\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "\n",
    "\n",
    "log_reg_cf = confusion_matrix(y_test, y_pred_log_reg)\n",
    "kneighbors_cf = confusion_matrix(y_test, y_pred_knear)\n",
    "svc_cf = confusion_matrix(y_test, y_pred_svc)\n",
    "tree_cf = confusion_matrix(y_test, y_pred_tree)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2,figsize=(22,12))\n",
    "\n",
    "\n",
    "sns.heatmap(log_reg_cf, ax=ax[0][0], annot=True, cmap=plt.cm.copper)\n",
    "ax[0, 0].set_title(\"Logistic Regression \\n Confusion Matrix\", fontsize=14)\n",
    "ax[0, 0].set_xticklabels(['', ''], fontsize=14, rotation=90)\n",
    "ax[0, 0].set_yticklabels(['', ''], fontsize=14, rotation=360)\n",
    "\n",
    "sns.heatmap(kneighbors_cf, ax=ax[0][1], annot=True, cmap=plt.cm.copper)\n",
    "ax[0][1].set_title(\"KNearsNeighbors \\n Confusion Matrix\", fontsize=14)\n",
    "ax[0][1].set_xticklabels(['', ''], fontsize=14, rotation=90)\n",
    "ax[0][1].set_yticklabels(['', ''], fontsize=14, rotation=360)\n",
    "\n",
    "sns.heatmap(svc_cf, ax=ax[1][0], annot=True, cmap=plt.cm.copper)\n",
    "ax[1][0].set_title(\"Suppor Vector Classifier \\n Confusion Matrix\", fontsize=14)\n",
    "ax[1][0].set_xticklabels(['', ''], fontsize=14, rotation=90)\n",
    "ax[1][0].set_yticklabels(['', ''], fontsize=14, rotation=360)\n",
    "\n",
    "sns.heatmap(tree_cf, ax=ax[1][1], annot=True, cmap=plt.cm.copper)\n",
    "ax[1][1].set_title(\"DecisionTree Classifier \\n Confusion Matrix\", fontsize=14)\n",
    "ax[1][1].set_xticklabels(['', ''], fontsize=14, rotation=90)\n",
    "ax[1][1].set_yticklabels(['', ''], fontsize=14, rotation=360)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "print('Logistic Regression:')\n",
    "print(classification_report(y_test, y_pred_log_reg))\n",
    "\n",
    "print('KNears Neighbors:')\n",
    "print(classification_report(y_test, y_pred_knear))\n",
    "\n",
    "print('Support Vector Classifier:')\n",
    "print(classification_report(y_test, y_pred_svc))\n",
    "\n",
    "print('Support Vector Classifier:')\n",
    "print(classification_report(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Score in the test set of logistic regression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Logistic Regression with Under-Sampling\n",
    "y_pred = log_reg.predict(X_test)\n",
    "undersample_score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "# Logistic Regression with SMOTE Technique (Better accuracy with SMOTE t)\n",
    "y_pred_sm = best_est.predict(original_Xtest)\n",
    "oversample_score = accuracy_score(original_ytest, y_pred_sm)\n",
    "\n",
    "\n",
    "d = {'Technique': ['Random UnderSampling', 'Oversampling (SMOTE)'], 'Score': [undersample_score, oversample_score]}\n",
    "final_df = pd.DataFrame(data=d)\n",
    "\n",
    "# Move column\n",
    "score = final_df['Score']\n",
    "final_df.drop('Score', axis=1, inplace=True)\n",
    "final_df.insert(1, 'Score', score)\n",
    "\n",
    "# Note how high is accuracy score it can be misleading! \n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks Testing Random UnderSampling Data vs OverSampling (SMOTE):\n",
    "In this section we will implement a simple Neural Network (with one hidden layer) in order to see which of the two logistic regressions models we implemented in the (undersample or oversample(SMOTE)) has a better accuracy for detecting fraud and non-fraud transactions. \n",
    "\n",
    "\n",
    "Our Main Goal:\n",
    "\n",
    "Our main goal is to explore how our simple neural network behaves in both the random undersample and oversample dataframes and see whether they can predict accuractely both non-fraud and fraud cases. Why not only focus on fraud? Imagine you were a cardholder and after you purchased an item your card gets blocked because the bank's algorithm thought your purchase was a fraud. That's why we shouldn't emphasize only in detecting fraud cases but we should also emphasize correctly categorizing non-fraud transactions.\n",
    "\n",
    "The Confusion Matrix:\n",
    "\n",
    "Here is again, how the confusion matrix works:\n",
    "\n",
    "Upper Left Square: The amount of correctly classified by our model of no fraud transactions.\n",
    "Upper Right Square: The amount of incorrectly classified transactions as fraud cases, but the actual label is no fraud .\n",
    "Lower Left Square: The amount of incorrectly classified transactions as no fraud cases, but the actual label is fraud .\n",
    "Lower Right Square: The amount of correctly classified by our model of fraud transactions.\n",
    "Summary (Keras || Random UnderSampling):\n",
    "\n",
    "Dataset: In this final phase of testing we will fit this model in both the random undersampled subset and oversampled dataset (SMOTE) in order to predict the final result using the original dataframe testing data.\n",
    "Neural Network Structure: As stated previously, this will be a simple model composed of one input layer (where the number of nodes equals the number of features) plus bias node, one hidden layer with 32 nodes and one output node composed of two possible results 0 or 1 (No fraud or fraud).\n",
    "Other characteristics: The learning rate will be 0.001, the optimizer we will use is the AdamOptimizer, the activation function that is used in this scenario is \"Relu\" and for the final outputs we will use sparse categorical cross entropy, which gives the probability whether an instance case is no fraud or fraud (The prediction will pick the highest probability between the two.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "\n",
    "n_inputs = X_train.shape[1]\n",
    "\n",
    "undersample_model = Sequential([\n",
    "    Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
